{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a97a227f-423f-49b3-a837-3d4541c40872",
   "metadata": {},
   "source": [
    "### Oracle AI Vector Search: Loading the Vector Store\n",
    "\n",
    "With this Notebook you can load your Knowledge Base in Oracle DB and create and  store the Embeddings Vectors.\n",
    "\n",
    "The KB is made by a set of txt files, stored in the txt directory. This NB:\n",
    "* Reads all the txt files and splits in chunks\n",
    "* Compute the embeddings for all chunks\n",
    "* Store chunks and embeddings in the ORACLE_KNOWLEDGE table\n",
    "\n",
    "* This demo is based on the **LangChain** integration\n",
    "* based on **OCI GenAI multi-lingual (Cohere) embeddings**\n",
    "* Data will be stored in a single table (ORACLE_KNOWLEDGE)\n",
    "\n",
    "Afterward, you can do a similarity search and run a simple assistant, based on OCI GenAI, on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f64d9fd0-36d2-43bf-b59c-b25001673ebc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "# to load and split txt documents\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# to compute embeddings vectors\n",
    "from langchain_community.embeddings import OCIGenAIEmbeddings\n",
    "\n",
    "# the class to integrate OCI AI Vector Search with LangChain\n",
    "from oracle_vector_db_lc import OracleVectorStore\n",
    "\n",
    "from config_private import COMPARTMENT_OCID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924d39f9-d1c4-4c0f-8d1a-a9c7bc9fcbf8",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "526385c8-16d1-4c4e-a358-47d246e98a10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Helper functions\n",
    "#\n",
    "\n",
    "\n",
    "# find the url from the file name, using references.csv\n",
    "def find_ref(df, f_name):\n",
    "    condition = df[\"file_name\"] == f_name\n",
    "\n",
    "    ref = df.loc[condition][\"url\"].values[0]\n",
    "\n",
    "    return ref\n",
    "\n",
    "\n",
    "# this function replace the file name with the url in docs metadata\n",
    "# the url is read from references.csv\n",
    "def set_url_in_docs(docs, df_ref):\n",
    "    docs = docs.copy()\n",
    "    for doc in docs:\n",
    "        # remove txt from file_name\n",
    "        file_name = doc.metadata[\"source\"]\n",
    "        only_name = file_name.split(\"/\")[-1]\n",
    "        # find the url from the csv\n",
    "        ref = find_ref(df_ref, only_name)\n",
    "\n",
    "        doc.metadata[\"source\"] = ref\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8da1d7c-25b0-4e7d-9d5f-ec6b1529cd0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Some configurations\n",
    "#\n",
    "\n",
    "# directory where our Knowledge base is contained in txt files\n",
    "TXT_DIR = \"./txt\"\n",
    "# file with f_name, url\n",
    "REF_FILE = \"references.csv\"\n",
    "\n",
    "# OCI GenAI model used for Embeddings\n",
    "EMBED_MODEL = \"cohere.embed-multilingual-v3.0\"\n",
    "ENDPOINT = \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\"\n",
    "\n",
    "# max length in token of the input for embeddings\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# max chunk size, in char, for splitting\n",
    "CHUNK_SIZE = 1500\n",
    "# this parameters needs to be adjusted for the Embed model (for example, lowered for Cohere)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ef2fd13-0ac8-4a9c-acbf-4a2174bca928",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 75 files to be loaded...\n"
     ]
    }
   ],
   "source": [
    "# this is the file list containing the Knowledge base\n",
    "file_list = sorted(glob(TXT_DIR + \"/\" + \"*.txt\"))\n",
    "\n",
    "print(f\"There are {len(file_list)} files to be loaded...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3801e226-e346-4836-b35a-94e15d28e695",
   "metadata": {},
   "source": [
    "#### Load all text files and then splits in chunks\n",
    "Here we do some preprocessing on the txt file:\n",
    "* we replace the file_name in source with the url the txt is coming from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76563e53-5dfc-45d7-b6a7-87f9a3606398",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 75/76 [00:00<00:00, 4034.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have splitted docs in 437 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# read all references (url) from  csv file\n",
    "df_ref = pd.read_csv(REF_FILE)\n",
    "\n",
    "# load txt and splits in chunks\n",
    "# with TextLoader it is fast\n",
    "# documents not yet splitted\n",
    "origin_docs = DirectoryLoader(\n",
    "    TXT_DIR, glob=\"**/*.txt\", show_progress=True, loader_cls=TextLoader\n",
    ").load()\n",
    "\n",
    "# replace the f_name with the reference (url)\n",
    "origin_docs = set_url_in_docs(origin_docs, df_ref)\n",
    "\n",
    "# split docs in chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # thse params must be adapted to Knowledge base\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "docs_splitted = text_splitter.split_documents(origin_docs)\n",
    "\n",
    "print(f\"We have splitted docs in {len(docs_splitted)} chunks...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518a854c-ba3b-4db3-99d0-ce2e4d5ff8db",
   "metadata": {},
   "source": [
    "#### Create Embed Model, Vector Store and load vectors + embeddings in the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1eb43ac-1975-4091-a691-f42ce8163ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 16:48:19,325 - INFO - ORACLE_KNOWLEDGE dropped!!!\n"
     ]
    }
   ],
   "source": [
    "# clean the existing table\n",
    "# be careful: do you really want to delete all the existing records?\n",
    "OracleVectorStore.drop_collection(collection_name=\"ORACLE_KNOWLEDGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39f129df-1cb1-4b33-963a-4c4a7d73528f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 16:48:20,973 - INFO - ORACLE_KNOWLEDGE created!!!\n"
     ]
    }
   ],
   "source": [
    "OracleVectorStore.create_collection(collection_name=\"ORACLE_KNOWLEDGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6feaa50-d4e8-4f65-9b10-74fc3cc467fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 16:48:22,347 - INFO - Compute embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12967d3ce1964a728c41eccbcd7d24cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 16:48:35,063 - INFO - Saving texts, embeddings to DB...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3518739b698746fd927539f2f17fc8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/437 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 16:48:46,904 - INFO - Tot. errors in save_embeddings: 0\n"
     ]
    }
   ],
   "source": [
    "# create embedding model and then the vector store\n",
    "\n",
    "# create the Embedding Model\n",
    "embed_model = OCIGenAIEmbeddings(\n",
    "    # this code is done to be run in OCI DS.\n",
    "    # If outside replace with API_KEY and provide API_KEYS\n",
    "    # auth_type = \"RESOURCE_PRINCIPAL\"\n",
    "    auth_type=\"API_KEY\",\n",
    "    model_id=EMBED_MODEL,\n",
    "    service_endpoint=ENDPOINT,\n",
    "    compartment_id=COMPARTMENT_OCID,\n",
    ")\n",
    "\n",
    "# Here compute embeddings and load texts + embeddings in DB\n",
    "# can take minutes (for embeddings)\n",
    "v_store = OracleVectorStore.from_documents(\n",
    "    docs_splitted, embed_model, collection_name=\"ORACLE_KNOWLEDGE\", verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7333cb53-74db-4928-9d40-9bf046d6a7f6",
   "metadata": {},
   "source": [
    "#### Do a query for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9553485-4cfa-4da3-89b2-f8431206b3f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# k is the number of docs we want to retrieve\n",
    "retriever = v_store.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d62e60a8-114f-4377-bcca-aeaa93c2b2bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 16:48:49,464 - INFO - top_k: 5\n",
      "2024-02-27 16:48:49,465 - INFO - \n",
      "2024-02-27 16:48:50,480 - INFO - select: select C.id, C.CHUNK, C.REF, \n",
      "                            ROUND(VECTOR_DISTANCE(C.VEC, :1, DOT), 3) as d \n",
      "                            from ORACLE_KNOWLEDGE C\n",
      "                            order by d\n",
      "                            FETCH FIRST 5 ROWS ONLY\n",
      "2024-02-27 16:48:50,733 - INFO - Query duration: 0.4 sec.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is Oracle Strategy for Generative AI?\"\n",
    "\n",
    "result_docs = retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c960e460-3182-4586-9cec-2e83870ffbc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I mentioned earlier that we took a holistic approach to generative AI as we thought through the complete picture of what enterprises truly need to successfully implement generative AI. But beyond that, we have some core tenets to help ensure that our new offerings will be as valuable as possible for you:\n",
      "We’re providing enterprise-focused models that are high performing and cost-effective, allowing for many uses cases and efficient fine tuning. We’re also increasingly adapting models to real-world enterprise scenarios, and performing specialized training on large language models with Oracle’s own proprietary knowledge and insights to make them better for business—all with access to best-in-class GPUs and high-performance cluster networking.\n",
      "Oracle meets you where you are in your generative AI journey with a variety of embedded and managed services features across our infrastructure layer, platform services, and business applications. Working with AI may seem challenging—but it’s dramatically simpler if you’re working with a company that has created an entire cloud of integrated services from data to apps.\n",
      "https://blogs.oracle.com/ai-and-datascience/post/future-generative-ai-what-enterprises-need-to-know\n",
      "----------------------------\n",
      "\n",
      "Not only is Oracle delivering an integrated, seamless AI experience, but we’re also delivering it where you need it. You can use Oracle Cloud Infrastructure (OCI) Generative AI in the Oracle Cloud and leverage all the advantages of the public cloud to scale solutions on demand, customize models, and create private model endpoints for business. With OCI Dedicated Region, Oracle will also deliver generative AI services in your data centers, so you can combine generative AI capabilities together with your on-premises data and applications.\n",
      "As Dave Vellante, Chief Research Officer at Wikibon recently said, “Oracle is taking a full stack approach to enterprise generative AI. Oracle’s value starts at the top of the stack, not in silicon. By offering integrated generative AI across its Fusion SaaS applications, Oracle directly connects to customer business value. These apps are supported by autonomous databases with vector embeddings and run on high-performance infrastructure across OCI or on-prem with Dedicated Region. Together these offerings comprise a highly differentiated enterprise AI strategy, covering everything from out-of-the-box RAG to a broad range of fine-tuned models and AI infused throughout an integrated stack. Our research shows that 2023 was the year of AI experimentation. With capabilities such as this, our expectation is that 2024 will be the year of showing ROI in AI.”\n",
      "https://blogs.oracle.com/ai-and-datascience/post/future-generative-ai-what-enterprises-need-to-know\n",
      "----------------------------\n",
      "\n",
      "-Holger Mueller, Vice President and Principal Analyst, Constellation Research\n",
      "Wikibon\n",
      "“Oracle is taking a full stack approach to enterprise generative AI. Oracle’s value starts at the top of the stack, not in silicon. By offering integrated generative AI across its Fusion SaaS applications, Oracle directly connects to customer business value. These apps are supported by autonomous databases with vector embeddings and run on high-performance infrastructure across OCI or on-prem with Dedicated Region. Together these offerings comprise a highly differentiated enterprise AI strategy, covering everything from out-of-the-box RAG to a broad range of fine-tuned models and AI infused throughout an integrated stack. Our research shows that 2023 was the year of AI experimentation. With capabilities such as this, our expectation is that 2024 will be the year of showing ROI in AI.”\n",
      "-Dave Vellante, Chief Research Officer, Wikibon\n",
      "“The new OCI Generative AI service from Oracle is seamlessly integrated up and down the entire stack from the hardware through the applications—making the massive “Rube Goldberg”-like effort of integrating generative AI into mission-critical workloads a thing of the past. Suppose you want to bake a cake; with other cloud providers you go to a grocery store, buy all the ingredients and they provide the mixing bowl, pan, and oven to bake it in. But they don’t even provide the recipe. In contrast, OCI provides you the entire gen AI cake—already baked.\"\n",
      "https://blogs.oracle.com/ai-and-datascience/post/oci-generative-ai-experts-saying\n",
      "----------------------------\n",
      "\n",
      "“With today’s news, Oracle is bringing generative AI to customer workloads and their data—not asking customers to move their data to a separate vector database. With a common architecture for generative AI that is being integrated across the Oracle ecosystem from its Autonomous Database to Fusion SaaS applications, Oracle is bringing generative AI to where exabytes of customer data already reside, both in cloud data centers and on-premises environments. This greatly simplifies the process for organizations to deploy generative AI with their existing business operations.”\n",
      "-Ritu Jyoti, Group Vice President, Worldwide Artificial Intelligence and Automation Research Practice and Global AI Research Lead, IDC\n",
      "Omdia\n",
      "https://blogs.oracle.com/ai-and-datascience/post/oci-generative-ai-experts-saying\n",
      "----------------------------\n",
      "\n",
      "Oracle’s OCI Generative AI service and OCI AI Agents RAG service, released January 2024, are enterprise-grade offerings that enables organizations to build and deploy business-centric generative AI solutions securely and at scale. Oracle is integrating generative AI broadly across its entire product portfolio. With its full-stack architecture spanning chip design to applications, Oracle is delivering end-to-end, cost-effective generative AI solutions focused on business value.\n",
      "Industry analysts have provided the following comments on Oracle’s new generative AI services.\n",
      "IDC\n",
      "“Organizations everywhere have struggled with how to deliver generative AI successfully—and Oracle just provided the answer. With its new Generative AI service, which supports fine tuning so businesses can customize LLMs to their own internal operations, and a comprehensive AI strategy that spans all layers of its tech stack, Oracle has demonstrated that it's focused on solving real world business problems.”\n",
      "- David Schubmehl, Research Vice President, Conversational Artificial Intelligence and Intelligent Knowledge Discovery, IDC\n",
      "https://blogs.oracle.com/ai-and-datascience/post/oci-generative-ai-experts-saying\n",
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in result_docs:\n",
    "    print(doc.page_content)\n",
    "    print(doc.metadata[\"source\"])\n",
    "    print(\"----------------------------\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f105a80-3cc3-4c2c-8676-163f31a98252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
